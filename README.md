# DA-in-GameDev-lab5
Отчет по лабораторной работе #5 выполнил(а):
- Фефелов Василий Владимирович
- РИ-220933
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 100 |
| Задание 2 | * | 100 |
| Задание 3 | * | 100 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

## Цель работы
Ознакомиться с основными функциями языка Python и игрового движка Unity 

Задание 1. Сделайте 3, 9, 27 копий модели «Плоскость-Сфера-Куб», запустите симуляцию сцены и наблюдайте за результатом обучения модели.
После завершения обучения проверьте работу модели.
Сделайте выводы.
* Модель работает, обучение происходит. Сильно ресурсозатратно, но до 9 копий модели все работает корректно. При 27 копиях выдает ошибку "The Unity environment took too long to respond"
* Шарики учатся находить и обнимать кубик)
* Изменил размер буфера до 1024
![image](https://github.com/VFlov/DA-in-GameDev-lab5/assets/129610413/cd6e96da-9592-4466-a733-07607b839179)

Задание 2. Симулятор добычи ресурсов
![image](https://github.com/VFlov/DA-in-GameDev-lab5/assets/129610413/332acaa6-44cf-4cc0-8144-78916a8f0247)
![image](https://github.com/VFlov/DA-in-GameDev-lab5/assets/129610413/a909a42c-a130-4a1d-a166-d1cc1c171035)

Пункт 1. Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.
Определяет, как одна переменная изменяется по отношению к другой. Например Value Loss - Средняя потеря при обновлении функции ценности. Коррелирует с тем, насколько насколько хорошо модель способна предсказать значение каждого состояния.

Пункт 2. Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.
* Cumulative Reward - Среднее кумулятивное вознаграждение за эпизод для всех агентов. Должно увеличиваться во время успешной тренировки.

* Entropy - Насколько случайны решения модели. Должен медленно уменьшаться во время успешного процесса обучения. Если она уменьшается слишком быстро, то гиперпараметр beta гиперпараметр должен быть увеличен

* Episode Length - Средняя продолжительность каждого эпизода в среде для всех агентов

* Learning Rate - Насколько большой шаг делает алгоритм обучения при поиске для поиска оптимальной политики. Должен уменьшаться со временем

* Policy Loss - Средняя потеря при обновлении функции политики. Коррелирует с тем, насколько сильно меняется политика (процесс принятия решений о действиях). Величина этого показателя должна уменьшаться во время успешной тренировки

* Value Estimate - Оценка среднего значения для всех состояний, которые посетил агент. Должна увеличиваться во время успешной тренировки.
Задание 3 - Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 
Монотонные задачи, которые требуют простого решения: Пройти лабиринт, собирательство ресурсов(в играх), написание простеньких сценатриев, текстов и многое другое


## Выводы

Абзац умных слов о том, что было сделано и что было узнано.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
